{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import json\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "import shutil\n",
    "\n",
    "from nemo.collections.tts.models.speechllm.megatron_t5_speechllm_model import MegatronT5SpeechLMModel\n",
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronTrainerBuilder\n",
    "from nemo.collections.asr.parts.preprocessing.segment import AudioSegment\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils import logging\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "\n",
    "# CHANGE THIS TO A LOCAL DIRECTORY\n",
    "EXP_DIR = \"/datap/misc/NotebookInference\"\n",
    "\n",
    "if not os.path.exists(EXP_DIR):\n",
    "    os.makedirs(EXP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfa55a",
   "metadata": {},
   "source": [
    "## Save a dummy manifest to setup Model Test Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_records(fp, records):\n",
    "    with open(fp, \"w\") as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "dummy_codes = torch.ones(8, 300).cpu().type(torch.int16)\n",
    "dummy_codes_fp = os.path.join(EXP_DIR, \"dummy_codes.pt\")\n",
    "torch.save(dummy_codes, dummy_codes_fp)\n",
    "\n",
    "\n",
    "dummy_record = {\n",
    "    \"question\" : \"Phoneme TTS Sample Text\",\n",
    "    \"answer\" : dummy_codes_fp,\n",
    "    \"context\" : dummy_codes_fp,\n",
    "    \"context_type\" : \"REFSPEAKERCODEC\",\n",
    "    \"question_type\" : \"TEXT\",\n",
    "    \"answer_type\" : \"AUDIOCODEC\",\n",
    "    \"context_duration\" : 5.0,\n",
    "    \"answer_duration\" : 5.0,\n",
    "    \"taskname\" : \"squad\"\n",
    "}\n",
    "\n",
    "dummy_val_file = os.path.join(EXP_DIR, \"dummy_val.json\")\n",
    "\n",
    "write_records(dummy_val_file, [dummy_record])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd90c5",
   "metadata": {},
   "source": [
    "## Load and setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1df6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THESE PATHS TO RELEVANT MOUNTED PATHS IN DOCKER\n",
    "config_path = \"/home/pneekhara/2023/NeMo/examples/tts/speechllm/conf/megatron_t5_speechllm_inference_multiencoder.yaml\"\n",
    "# checkpoint_path = \"/datap/misc/temp_checkpoints_new/desta_less_sophia_highLR_step159600.ckpt\"\n",
    "checkpoint_path = \"/datap/misc/checkpoints/desta_less_sophia_213850.ckpt\"\n",
    "codecmodel_path = \"/datap/misc/checkpoints/SpeechCodec_2402.nemo\"\n",
    "vocab_file = \"/datap/misc/checkpoints/9a77f10c2793465e8e8a3fa5fcbef8b0_vocab.txt\"\n",
    "\n",
    "cfg = OmegaConf.load(config_path)\n",
    "\n",
    "if \"gradient_as_bucket_view\" not in cfg.model:\n",
    "    with open_dict(cfg):\n",
    "        cfg.model.gradient_as_bucket_view=False\n",
    "\n",
    "trainer = MegatronTrainerBuilder(cfg).create_trainer()\n",
    "exp_manager(trainer, cfg.exp_manager)\n",
    "\n",
    "with open_dict(cfg):\n",
    "    cfg.exp_manager.exp_dir=EXP_DIR\n",
    "    cfg.checkpoint_path = checkpoint_path\n",
    "    cfg.model.data.sup_data_path=\"/datap/misc/speechllm_codecdatasets/\"\n",
    "    cfg.model.global_batch_size=1\n",
    "    cfg.model.micro_batch_size=1\n",
    "    cfg.model.data.speech_offset=30128\n",
    "    cfg.model.lm_vocab_size=30000\n",
    "    cfg.model.data.add_special_tokens_to_only_first_codebook=True\n",
    "    cfg.model.data.train_task=\"all\"\n",
    "    cfg.model.freeze_model=False\n",
    "    cfg.model.data.max_seq_length=2048\n",
    "    cfg.model.max_inference_timesteps=2000\n",
    "    cfg.model.data.context_duration_min=20.0\n",
    "    cfg.model.data.context_duration_max=20.0\n",
    "    cfg.model.top_k=80\n",
    "    cfg.model.temperature=0.85\n",
    "    cfg.model.data.speech_offset=30128\n",
    "    cfg.model.lm_vocab_size=30000\n",
    "    cfg.model.codecmodel_path=codecmodel_path\n",
    "    cfg.trainer.devices=1\n",
    "    cfg.trainer.precision=\"bf16\"\n",
    "    cfg.model.precision = cfg.trainer.precision\n",
    "    cfg.model.override_tokenizer_vocab_file=vocab_file\n",
    "    cfg.model.english_only_model=True\n",
    "    cfg.model.asr_model_name=\"stt_en_conformer_transducer_large\"\n",
    "    cfg.model.frozen_model.decoder.layer_type=[1,1,1,2,2,2,2,2,2,2,1,1]\n",
    "    cfg.model.alignment_decoder_layerids=[0,1,2,3,4]\n",
    "    cfg.model.enc_output_to_layers=[[8,9],[3,4,5,6,7]]\n",
    "    cfg.model.data.test_ds=[dummy_val_file]\n",
    "    cfg.model.data.num_workers = 0\n",
    "\n",
    "\n",
    "checkpoint_path = cfg.get('checkpoint_path', None)\n",
    "assert checkpoint_path is not None, \"checkpoint path needs to be valid\"\n",
    "\n",
    "model = MegatronT5SpeechLMModel.load_from_checkpoint(\n",
    "        checkpoint_path=checkpoint_path, trainer=trainer, cfg=cfg.model\n",
    "    )\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "codec_model = model.additional_models['codec']\n",
    "trainer.test(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5461918",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join( model.trainer.logger.save_dir, model.trainer.logger.name, model.trainer.logger.version, \"Sample_Audios\")\n",
    "out_path = os.path.join(out_dir, 'predicted_wav_0.wav')\n",
    "\n",
    "\n",
    "def encode(wav_path):\n",
    "    # Convert an audio file to nemo codec codes\n",
    "    features = AudioSegment.segment_from_file(\n",
    "                    wav_path, target_sr=codec_model.sample_rate, n_segments=-1, trim=False,\n",
    "                )\n",
    "    audio_samples = features.samples\n",
    "    audio = torch.tensor(audio_samples).cuda()\n",
    "    audio_length = torch.tensor(audio.size(0)).long().cuda()\n",
    "    print(f\"audio {audio.size()} audio_length {audio_length}\")\n",
    "    print(f\"audio {audio.device} audio_length {audio_length.device} codec_model {codec_model.device}\")\n",
    "\n",
    "    original_codec_codes, _ = codec_model.encode(audio=audio.unsqueeze(0), audio_len=audio_length.unsqueeze(0))\n",
    "    original_codec_codes = original_codec_codes[0]\n",
    "    print(f\"original_codec_codes {original_codec_codes.size()} audio {audio.size()} audio_length {audio_length}\")\n",
    "    duration = original_codec_codes.size()[1] / 86\n",
    "    \n",
    "    target_codec_filepath = wav_path[:-4] + \"_codes.pt\"\n",
    "    torch.save(original_codec_codes.cpu().type(torch.int16), target_codec_filepath)\n",
    "    return original_codec_codes, target_codec_filepath, duration\n",
    "    \n",
    "    \n",
    "    \n",
    "def play_codec(codec_path):\n",
    "    # Convert nemo codecs to audio and play it\n",
    "    codec = torch.load(codec_path)\n",
    "    codec = codec.to('cuda')\n",
    "    codec = codec.unsqueeze(0)\n",
    "    codec_lens = torch.Tensor([codec.shape[2]]).long().cuda()\n",
    "    codec_decoded_audios, _ = codec_model.decode(tokens=codec.long(), tokens_len=codec_lens)\n",
    "    codec_decoded_audio = codec_decoded_audios[0]\n",
    "    temp_wav_path = os.path.join(EXP_DIR, \"temp.wav\")\n",
    "    torchaudio.save(temp_wav_path, codec_decoded_audio[None].cpu(), 22050)\n",
    "    display(Audio(temp_wav_path))\n",
    "\n",
    "def generate_new_audio(\n",
    "    text,\n",
    "    context,\n",
    "    context_duration=4.0,\n",
    "    context_type=\"REFSPEAKERCODEC\",\n",
    "    temperature=0.85,\n",
    "    top_k=80,\n",
    "    text_task=\"Phoneme TTS \"\n",
    "    ):\n",
    "    # Prepare data in speechllm format\n",
    "    model.cfg.temperature = temperature\n",
    "    model.cfg.top_k = top_k\n",
    "    dummy_answer = dummy_codes_fp\n",
    "    json_in = {}\n",
    "    json_in[\"question\"] = text_task + text\n",
    "    json_in[\"question_type\"] = \"TEXT\"\n",
    "    json_in[\"answer\"] = dummy_answer \n",
    "    json_in[\"context\"] = context \n",
    "    json_in[\"answer_type\"] = \"AUDIOCODEC\"\n",
    "    json_in[\"context_type\"] = context_type\n",
    "    json_in[\"context_duration\"] = context_duration\n",
    "    json_in[\"answer_duration\"] = 2.0\n",
    "    json_in[\"taskname\"] = \"squad\"\n",
    "    json_in[\"lang\"] = \"en\"\n",
    "    json_in = [json_in]\n",
    "    \n",
    "    # Prepare dataloader\n",
    "    model._test_ds.examples = []\n",
    "    model._test_ds.examples = model._test_ds.load_data(json_in)\n",
    "    \n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            model._test_ds, num_replicas=1, rank=0, shuffle=False, seed=1\n",
    "        )\n",
    "\n",
    "    model._test_dl = torch.utils.data.DataLoader(\n",
    "        model._test_ds,\n",
    "        collate_fn=model._test_ds.collate_fn,\n",
    "        sampler=sampler,\n",
    "        batch_size=1,\n",
    "        drop_last=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=False,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # Run inference\n",
    "    model.cfg.data.test_ds = None\n",
    "    trainer.test(model, model._test_dl)\n",
    "    print(\"Out path:\", out_path)\n",
    "    print(\"Inference done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f449a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_contexts = [\n",
    "    \"TEXT CONTEXT: | Language:en Dataset:Riva Speaker:Lindy_CMU_FEARFUL |\",\n",
    "    \"TEXT CONTEXT: | Language:en Dataset:Riva Speaker:Lindy_CMU_HAPPY |\",\n",
    "    \"TEXT CONTEXT: | Language:en Dataset:Riva Speaker:Rodney_CMU_HAPPY |\",\n",
    "    \"TEXT CONTEXT: | Language:en  Dataset:PromptTTS Gender:female SpeakingRate:2. Slow emotion:neutral Pitch:4. High SNR:5. Clean REVERB:5. Very close-sounding |\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5a467",
   "metadata": {},
   "source": [
    "## Generate audio from a text context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9660d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"As I closed my laptop for the night, my reflection in the screen continued to smile back at me.\"\n",
    "text_task = \"Phoneme TTS \" # Can be \"Text to speech this \" (for sentence-piece tokenizer) or \"Phoneme TTS \" (for phoneme tokenizer)\n",
    "context = text_contexts[1] # Sample Text Context\n",
    "context_type = \"TEXT\" # Can be REFSPEAKERCODEC (for audio context), TEXT (for text context)\n",
    "generate_new_audio(\n",
    "    text, \n",
    "    context, \n",
    "    context_type=context_type, \n",
    "    context_duration=5.0, # Does not matter, should just be > 3 so that dataset does not filter it out.\n",
    "    top_k=80, # Can play around with this to check roubstness\n",
    "    temperature=0.8, # Can play around with this. temperature < 0.85 can be more robust\n",
    "    text_task=text_task\n",
    ")\n",
    "display(Audio(out_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a964c3",
   "metadata": {},
   "source": [
    "## Listen to some ground-truth context audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_paths = [\n",
    "    \"/datap/misc/speechllm_codecdatasets/codecs/RivattsAllLanguagesUpdated_train_nemo_codec_bw_6.0/target_codes_en_Lindy_44khz_CMU_HAPPY_LINDY_CMU_HAPPY_000570.pt\",\n",
    "]\n",
    "\n",
    "for cidx, context_path in enumerate(context_paths):\n",
    "    print(cidx, context_path)\n",
    "    play_codec(context_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8bc28",
   "metadata": {},
   "source": [
    "## Generate audio from an audio context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"As I closed my laptop for the night, my reflection in the screen continued to smile back at me.\"\n",
    "text_task = \"Text to speech this \" # Can be \"Text to speech this \" (for sentence-piece tokenizer) or \"Phoneme TTS \" (for phoneme tokenizer)\n",
    "context = context_paths[0] # Sample Text Context\n",
    "context_type = \"REFSPEAKERCODEC\" # Can be REFSPEAKERCODEC (for audio context), TEXT (for text context)\n",
    "generate_new_audio(\n",
    "    text, \n",
    "    context, \n",
    "    context_type=context_type, \n",
    "    context_duration=5.0, # Does not matter, should just be > 3 so that dataset does not filter it out.\n",
    "    temperature=0.8, # Can play around with this. temperature < 0.85 can be more robust\n",
    "    text_task=text_task\n",
    ")\n",
    "display(Audio(out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d2450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
